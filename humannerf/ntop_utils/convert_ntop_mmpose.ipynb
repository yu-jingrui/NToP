{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2faa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d080a9e",
   "metadata": {},
   "source": [
    "# 0. Naming convention\n",
    "\n",
    "coco uses running number for the files, but we don't have this, so maybe like this:\n",
    "\n",
    "| | origin dataset | subject | action | origin image | H | R | sub image | file extension |\n",
    "| -- | -- | --- | --- | --- | --- | -- | -- | -- |\n",
    "|Image:| 0dd | 0ss | 0aa | xxxxxx | hh | rr | i | .png |\n",
    "|Anno: | 0dd | 0ss | 0aa | xxxxxx | hh | rr | i | .json |\n",
    "\n",
    "**Example images:**\n",
    "\n",
    "Human3.6M - S1 - Directions - frame 1055 - H1.2R1.0 - center: `001 001 001 001055 12 10 0.png`\n",
    "\n",
    "Genebody - Ahha - NaN - frame 123 - H1.2R1.0 - east: `002 001 000 000123 12 10 1.png`\n",
    "\n",
    "Genebody - Ahha - NaN - frame 85 - H1.8R1.0 - south: `002 001 000 000085 18 10 3.png`\n",
    "\n",
    "**Annotations:**\n",
    "\n",
    "For annotations the current convention is one .json file for each subject-action\n",
    "\n",
    "Human3.6M - S1 - Directions: `001 001 001 xxxxxx 12 10 i.json`\n",
    "\n",
    "Genebody - Ahha - NaN - all frames - H1.2R1.0 - all positions: `002 001 000 xxxxxx 12 10 i.json`\n",
    "\n",
    "Genebody - Ahha - NaN - all frames - H1.0R0.5 - south: `002 001 000 xxxxxx 10 05 i.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2ac58",
   "metadata": {},
   "source": [
    "# Y. Generate coco format pose annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_cocolike_anno import create_cocolike_pose_anno\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "genebody_subjects = ['ahha', 'alejandro', 'anastasia', 'aosilan', 'arslan',\n",
    "                     'barlas', 'barry', 'camilo', 'dannier', 'gaoxing',\n",
    "                     'huajiangtao5', 'joseph', 'kamal_ejaz', 'kemal',\n",
    "                     'lihongyun', 'natacha', 'quyuanning', 'rabbi', 'rivera',\n",
    "                     'songyujie', 'sunyuxing', 'wuwenyan', 'xujiarui',\n",
    "                     'zhanghao', 'zhanghongwei', 'zhangziyu', 'zhuna2']\n",
    "human36m_subjects = ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11']\n",
    "zjumocap_subjects = ['p313', 'p315', 'p377', 'p386', 'p387', 'p390', 'p392', 'p393', 'p394']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = {'genebody': genebody_subjects,\n",
    "                'human36m': human36m_subjects,\n",
    "                'zjumocap': zjumocap_subjects,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'genebody'\n",
    "posetype = 'coco'\n",
    "for subj in genebody_subjects:\n",
    "    print('GMT ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())) + f' - Generating for {subset} : {subj}')\n",
    "    kp_annos, bbox_annos = create_cocolike_pose_anno(ntop_src, subset, subj, posetype=posetype)\n",
    "\n",
    "    pose_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_{posetype}.json')\n",
    "    with open(pose_anno_f, 'w') as f:\n",
    "        json.dump(kp_annos, f)\n",
    "    bbox_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox.json')\n",
    "    with open(bbox_anno_f, 'w') as f:\n",
    "        json.dump(bbox_annos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161915eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'human36m'\n",
    "for subj in human36m_subjects:\n",
    "    print('GMT ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())) + f' - Generating for {subset} : {subj}')\n",
    "    kp_annos, bbox_annos = create_cocolike_pose_anno(ntop_src, subset, subj, posetype=posetype)\n",
    "\n",
    "    pose_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_{posetype}.json')\n",
    "    with open(pose_anno_f, 'w') as f:\n",
    "        json.dump(kp_annos, f)\n",
    "    bbox_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox.json')\n",
    "    with open(bbox_anno_f, 'w') as f:\n",
    "        json.dump(bbox_annos, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'zjumocap'\n",
    "posetype = 'coco'\n",
    "for subj in zjumocap_subjects[:1]:\n",
    "    print('GMT ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())) + f' - Generating for {subset} : {subj}')\n",
    "    kp_annos, bbox_annos = create_cocolike_pose_anno(ntop_src, subset, subj, posetype=posetype)\n",
    "\n",
    "    pose_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_{posetype}.json')\n",
    "    with open(pose_anno_f, 'w') as f:\n",
    "        json.dump(kp_annos, f)\n",
    "    bbox_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox.json')\n",
    "    with open(bbox_anno_f, 'w') as f:\n",
    "        json.dump(bbox_annos, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46450246",
   "metadata": {},
   "source": [
    "## Y.2 Hybrik-like annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in images:\n",
    "    img_id_str = f\"00{img['id']}\"\n",
    "    if img_id_str not in raw_annos_everything.keys():\n",
    "        print(f'{image_id_str} not found in raw annos')\n",
    "        pass\n",
    "    raw = raw_annos_everything[img_id_str]\n",
    "\n",
    "    img['cam_idx'] = int(str(img['id'])[-1])\n",
    "    img['cam_param'] = raw['cam_param']\n",
    "    img['subject'] = subj\n",
    "    img['subject_idx'] = int(str(dataset_idx)+subj_idx)\n",
    "    img['action_name'] = raw['action_name']\n",
    "    img['action_idx'] = raw['ac_idx']\n",
    "\n",
    "for an in annotations:\n",
    "    img_id_str = f\"00{an['image_id']}\"\n",
    "    if img_id_str not in raw_annos_everything.keys():\n",
    "        print(f'{image_id_str} not found in raw annos')\n",
    "        pass\n",
    "    raw = raw_annos_everything[img_id_str]\n",
    "    \n",
    "    an['thetas'] = raw['thetas']\n",
    "    an['betas'] = raw['betas']\n",
    "    an['root_coord'] = raw['root_coord']\n",
    "    smpl_kps = raw['joints_3d']\n",
    "    smpl_kps = [j[0:3] for j in smpl_kps]\n",
    "    smpl_kps_flat = list(np.array(smpl_kps).flatten())\n",
    "    an['smpl_keypoints'] = smpl_kps_flat\n",
    "    del an['keypoints']\n",
    "    del an['num_keypoints']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_cocolike_anno import create_hybrik_pose_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('//path/to/concatenated/NToP/datasets/')\n",
    "subset = 'genebody'\n",
    "for subj in genebody_subjects[8:]:\n",
    "    print('GMT ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())) + f' - Generating for {subset} : {subj}')\n",
    "    hybrik_annos = create_hybrik_pose_anno(ntop_src, subset, subj)\n",
    "\n",
    "    hybrik_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_hybrik.json')\n",
    "    with open(hybrik_anno_f, 'w') as f:\n",
    "        json.dump(hybrik_annos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb959a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'human36m'\n",
    "for subj in human36m_subjects:\n",
    "    print('GMT ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())) + f' - Generating for {subset} : {subj}')\n",
    "    hybrik_annos = create_hybrik_pose_anno(ntop_src, subset, subj)\n",
    "\n",
    "    hybrik_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_hybrik.json')\n",
    "    with open(hybrik_anno_f, 'w') as f:\n",
    "        json.dump(hybrik_annos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ae038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'zjumocap'\n",
    "for subj in zjumocap_subjects[0:1]:\n",
    "    print('GMT ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(time.time())) + f' - Generating for {subset} : {subj}')\n",
    "    hybrik_annos = create_hybrik_pose_anno(ntop_src, subset, subj)\n",
    "\n",
    "    hybrik_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_hybrik.json')\n",
    "    with open(hybrik_anno_f, 'w') as f:\n",
    "        json.dump(hybrik_annos, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce620d9b",
   "metadata": {},
   "source": [
    "## Y.3. correct num_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'zjumocap'\n",
    "\n",
    "for subj in all_subjects[subset]:\n",
    "    coco_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_coco.json')\n",
    "    with open(coco_anno_f, 'r') as f:\n",
    "        coco_anno = json.load(f)\n",
    "    for an in coco_anno['annotations']:\n",
    "        an['num_keypoints'] = 13\n",
    "    with open(coco_anno_f, 'w') as f:\n",
    "        json.dump(coco_anno, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93748eb",
   "metadata": {},
   "source": [
    "## Y.4 Convert to xywh bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaee1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'zjumocap'\n",
    "subj = 'p313'\n",
    "coco_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_coco.json')\n",
    "with open(coco_anno_f, 'r') as f:\n",
    "    coco_anno = json.load(f)\n",
    "for an in coco_anno['annotations']:\n",
    "    bbox = an['bbox']\n",
    "    w = bbox[2] - bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "    bbox[2] = w\n",
    "    bbox[3] = h\n",
    "coco_anno_xywh_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_coco_xywh.json')\n",
    "with open(coco_anno_xywh_f, 'w') as f:\n",
    "    json.dump(coco_anno, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'genebody'\n",
    "\n",
    "for subj in all_subjects[subset]:\n",
    "    coco_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_coco.json')\n",
    "    with open(coco_anno_f, 'r') as f:\n",
    "        coco_anno = json.load(f)\n",
    "    for an in coco_anno['annotations']:\n",
    "        bbox = an['bbox']\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "        bbox[2] = w\n",
    "        bbox[3] = h\n",
    "    coco_anno_xywh_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_coco_xywh.json')\n",
    "    with open(coco_anno_xywh_f, 'w') as f:\n",
    "        json.dump(coco_anno, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ace76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'genebody'\n",
    "\n",
    "for subj in all_subjects[subset]:\n",
    "    bbox_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox.json')\n",
    "    with open(bbox_anno_f, 'r') as f:\n",
    "        bbox_anno = json.load(f)\n",
    "    for bb in bbox_anno:\n",
    "        bbox = bb['bbox']\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "        bbox[2] = w\n",
    "        bbox[3] = h\n",
    "    bbox_xywh_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox_xywh.json')\n",
    "    with open(bbox_xywh_f, 'w') as f:\n",
    "        json.dump(bbox_anno, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f3050",
   "metadata": {},
   "source": [
    "# Z. Final dataset concatenation\n",
    "\n",
    "H36M renders: 470169\n",
    " 7 subjects, 15 actions  - 5 sub for training 72%\n",
    "    human36m:S1 - 55071 annotations\n",
    "    human36m:S5 - 84888 annotations\n",
    "    human36m:S6 - 56034 annotations\n",
    "    human36m:S7 - 92331 annotations\n",
    "    human36m:S8 - 58905 annotations\n",
    "    human36m:S9 - 70281 annotations\n",
    "    human36m:S11 - 52659 annotations\n",
    "\n",
    "ZJU_Mocap renders: 27000\n",
    " 9 subjects - 7 subjects for training\n",
    "    zjumocap:p313 - 3600 annotations\n",
    "    zjumocap:p315 - 7200 annotations\n",
    "    zjumocap:p377 - 2052 annotations\n",
    "    zjumocap:p386 - 1944 annotations\n",
    "    zjumocap:p387 - 1944 annotations\n",
    "    zjumocap:p390 - 4230 annotations\n",
    "    zjumocap:p392 - 1944 annotations\n",
    "    zjumocap:p393 - 2376 annotations\n",
    "    zjumocap:p394 - 1710 annotations\n",
    "\n",
    "genebody renders: 72900\n",
    " 27 subjects - 19 subjects for training\n",
    "    (each) - 2700 annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genebody_subjects = ['ahha', 'alejandro', 'anastasia', 'aosilan', 'arslan',\n",
    "                     'barlas', 'barry', 'camilo', 'dannier', 'gaoxing',\n",
    "                     'huajiangtao5', 'joseph', 'kamal_ejaz', 'kemal',\n",
    "                     'lihongyun', 'natacha', 'quyuanning', 'rabbi', 'rivera',\n",
    "                     'songyujie', 'sunyuxing', 'wuwenyan', 'xujiarui',\n",
    "                     'zhanghao', 'zhanghongwei', 'zhangziyu', 'zhuna2']\n",
    "human36m_subjects = ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11']\n",
    "zjumocap_subjects = ['p313', 'p315', 'p377', 'p386', 'p387', 'p390', 'p392', 'p393', 'p394']\n",
    "\n",
    "all_subjects = {'human36m': human36m_subjects,\n",
    "                'genebody': genebody_subjects,\n",
    "                'zjumocap': zjumocap_subjects}\n",
    "\n",
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ac250",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_train = [('human36m', subj) for subj in human36m_subjects[:5]]\n",
    "ntop_train += [('genebody', subj) for subj in genebody_subjects[:19]]\n",
    "ntop_train += [('zjumocap', subj) for subj in zjumocap_subjects[:7]]\n",
    "\n",
    "ntop_val = [('human36m', subj) for subj in human36m_subjects[5:]]\n",
    "ntop_val += [('genebody', subj) for subj in genebody_subjects[19:]]\n",
    "ntop_val += [('zjumocap', subj) for subj in zjumocap_subjects[7:]]\n",
    "\n",
    "print(ntop_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ntop_final_annos(split='train', subset=None, bbox_type='xywh', anno_type='coco'):\n",
    "    if split == 'train':\n",
    "        anno_set = ntop_train\n",
    "        if subset == 'human36m':\n",
    "            anno_set = anno_set[:5]\n",
    "        elif subset == 'genebody':\n",
    "            anno_set = anno_set[5:24]\n",
    "        elif subset == 'zjumocap':\n",
    "            anno_set = anno_set[24:]\n",
    "    elif split == 'val':\n",
    "        anno_set = ntop_val\n",
    "        if subset == 'human36m':\n",
    "            anno_set = anno_set[:2]\n",
    "        elif subset == 'genebody':\n",
    "            anno_set = anno_set[2:10]\n",
    "        elif subset == 'zjumocap':\n",
    "            anno_set = anno_set[10:]\n",
    "\n",
    "    annotations = []\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    \n",
    "    for subset, subj in anno_set:\n",
    "        if anno_type == 'hybrik':\n",
    "            input_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_{anno_type}.json')\n",
    "        else:\n",
    "            input_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_{anno_type}_{bbox_type}.json')\n",
    "        with open(input_anno_f, 'r') as f:\n",
    "            input_anno = json.load(f)\n",
    "        annotations += input_anno['annotations']\n",
    "        images += input_anno['images']\n",
    "        \n",
    "        if anno_type != 'hybrik':\n",
    "            bbox_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox_{bbox_type}.json')\n",
    "            with open(bbox_anno_f, 'r') as f:\n",
    "                bbox_anno = json.load(f)\n",
    "            bboxes += bbox_anno\n",
    "            \n",
    "        \n",
    "    input_anno['images'] = images\n",
    "    input_anno['annotations'] = annotations\n",
    "\n",
    "    return input_anno, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_type = 'xywh'\n",
    "anno_type = 'coco'\n",
    "\n",
    "for split in ['val']:\n",
    "    for subset in ['ntop', 'human36m', 'genebody', 'zjumocap']:\n",
    "        kps_annos, bboxes = gen_ntop_final_annos(split, subset)\n",
    "        \n",
    "        with open(Path(ntop_src, 'ntop', 'annotations', f'{subset}_{split}_keypoints_{bbox_type}.json'), 'w') as f:\n",
    "            json.dump(kps_annos, f)\n",
    "\n",
    "        with open(Path(ntop_src, 'ntop', 'annotations', f'{subset}_{split}_bboxes_{bbox_type}.json'), 'w') as f:\n",
    "            json.dump(bboxes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_type = 'hybrik'\n",
    "for split in ['val', 'train']:\n",
    "    for subset in ['ntop', 'human36m', 'genebody', 'zjumocap']:\n",
    "        print(f'Concatenate annotation for {subset}_{split}')\n",
    "        kps_annos, _ = gen_ntop_final_annos(split, subset, anno_type=anno_type)\n",
    "        with open(Path(ntop_src, 'ntop', 'annotations', f'{subset}_{split}_{anno_type}_xyxy.json'), 'w') as f:\n",
    "            json.dump(kps_annos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_type = 'hybrik'\n",
    "for split in ['val', 'train']:\n",
    "    for subset in ['ntop', 'human36m', 'genebody', 'zjumocap']:\n",
    "        anno_f = Path(ntop_src, 'ntop', 'annotations', f'{subset}_{split}_{anno_type}_xyxy.json')\n",
    "        with open(anno_f, 'r') as f:\n",
    "            annos = json.load(f)\n",
    "        for an in annos['annotations']:\n",
    "            bbox = an['bbox']\n",
    "            w = bbox[2] - bbox[0]\n",
    "            h = bbox[3] - bbox[1]\n",
    "            bbox[2] = w\n",
    "            bbox[3] = h\n",
    "        anno_xywh_f = Path(ntop_src, 'ntop', 'annotations', f'{subset}_{split}_{anno_type}_xywh.json')\n",
    "        with open(anno_xywh_f, 'w') as f:\n",
    "            json.dump(annos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_type = 'hybrik'\n",
    "split = 'val'\n",
    "subset = 'genebody'\n",
    "kps_annos, bboxes = gen_ntop_final_annos(split, subset, anno_type=anno_type)\n",
    "\n",
    "print(kps_annos['annotations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "images = []\n",
    "for subset, subj in ntop_val:\n",
    "    coco_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_coco_xywh.json')\n",
    "    with open(coco_anno_f, 'r') as f:\n",
    "        coco_anno = json.load(f)\n",
    "    annotations += coco_anno['annotations']\n",
    "    images += coco_anno['images']\n",
    "        \n",
    "coco_anno['images'] = images\n",
    "coco_anno['annotations'] = annotations\n",
    "\n",
    "with open(Path(ntop_src, 'ntop', 'annotations', f'ntop_val_keypoints_xywh.json'), 'w') as f:\n",
    "    json.dump(coco_anno, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "images = []\n",
    "for subset, subj in ntop_val:\n",
    "    hybrik_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_hybrik.json')\n",
    "    with open(hybrik_anno_f, 'r') as f:\n",
    "        hybrik_anno = json.load(f)\n",
    "    annotations += hybrik_anno['annotations']\n",
    "    images += hybrik_anno['images']\n",
    "        \n",
    "hybrik_anno['images'] = images\n",
    "hybrik_anno['annotations'] = annotations\n",
    "\n",
    "with open(Path(ntop_src, 'ntop', 'annotations', 'ntop_val_hybrik.json'), 'w') as f:\n",
    "    json.dump(hybrik_anno, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273935d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "for subset, subj in ntop_train:\n",
    "    bbox_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_bbox_xywh.json')\n",
    "    with open(bbox_anno_f, 'r') as f:\n",
    "        bbox_anno = json.load(f)\n",
    "    bboxes += bbox_anno\n",
    "\n",
    "with open(Path(ntop_src, 'ntop', 'annotations', f'ntop_train_bboxes_xywh.json'), 'w') as f:\n",
    "    json.dump(bboxes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move rgb renders to train and val folder\n",
    "for subset, subj in ntop_train:\n",
    "    rgb_dir = f\"{ntop_src}/{subset}/concat/{subj}/rgb/\"\n",
    "    dst_dir = f\"{ntop_src}/ntop/train_images/\"\n",
    "    imgs = os.listdir(rgb_dir)\n",
    "    for im in imgs:\n",
    "        shutil.copy(rgb_dir+im, dst_dir+im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a0f01",
   "metadata": {},
   "source": [
    "# A. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7223b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_cocolike_anno import vis_skeleton_single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "subset = 'genebody'\n",
    "subj='natacha'\n",
    "posetype='coco'\n",
    "pose_anno_f = Path(ntop_src, subset, 'anno', f'{subj}_anno_{posetype}_xyxy.json')\n",
    "with open(pose_anno_f, 'r') as f:\n",
    "    kp_annos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for anno in kp_annos['annotations'][1134:1143]:\n",
    "    ids.append(anno['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738209ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [int(f'2016000{i:06d}12106') for i in range(150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [int(f'20160000000661005{i}') for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop_src = Path('/path/to/concatenated/NToP/datasets/')\n",
    "pose_anno_f = Path(ntop_src, 'genebody', 'hybrik_anno', 'genebody_train_hybrik_xyxy.json')\n",
    "with open(pose_anno_f, 'r') as f:\n",
    "    kp_annos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d507289",
   "metadata": {},
   "outputs": [],
   "source": [
    "annos = [an  for an in kp_annos['annotations'] if an['id'] in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397aacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "annos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno in annos:\n",
    "    kps = anno['smpl_joints_img']\n",
    "    kps2d = [[kps[i*2], kps[i*2+1]] for i in range(24)]\n",
    "    \n",
    "    bbox = anno['bbox']\n",
    "    \n",
    "    #img_f = Path(ntop_src, subset, 'concat', subj, 'rgb', f'00{anno[\"image_id\"]}.png')\n",
    "    img_id = anno['image_id']\n",
    "    #img_id += 102050\n",
    "    img_f = Path(Path('/mnt/data/yjin/NTOP/'), 'ntop', 'train_images', f'00{img_id}.png')\n",
    "    #vis_skeleton_single_image(str(img_f.resolve()), kps2d, bbox)\n",
    "    #fig = vis_skeleton_single_image(kps2d, str(img_f.resolve()))\n",
    "    fig = vis_skeleton_single_image(kps2d)\n",
    "    #skel_f = Path(f'/home/yjin/ntop_2d_fig/{img_id}.png')\n",
    "    #fig.savefig(skel_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_cocolike_anno import plotSkel2D, skeleton_tree\n",
    "\n",
    "def vis_skeleton_single_image(keypoints, image_path=None, bbox=None):\n",
    "    kpts2d = np.array(keypoints)\n",
    "\n",
    "    fig = plt.figure(figsize =[5,5])\n",
    "    ax = fig.add_subplot(111)\n",
    "    if image_path is not None:\n",
    "        img = cv2.imread(image_path)\n",
    "        #ax.imshow(img[..., ::-1])\n",
    "        H, W = img.shape[:2]\n",
    "    else:\n",
    "        H, W = 1000, 1000\n",
    "#    plotSkel2D(kpts2d, ax = ax)\n",
    "    plotSkel2D(kpts2d, skeleton_tree, ax, linewidth=2, alpha=1, max_range=1, thres=0.5)\n",
    "    if bbox is not None:\n",
    "        plot_bbox(bbox, ax)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_cocolike_anno import skeleton_tree, skel3dplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd889515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def skel3dplot(kps, config, ax = None, phi = 0, theta = 0, linewidth = 4, color = None, max_range = 1):\n",
    "    if kps.shape[1] == 17:\n",
    "        kps = kps.reshape(17,3)\n",
    "    else:\n",
    "        kps = kps.reshape(24,3)\n",
    "    kps = kps[:, [0,2,1]]\n",
    "    multi = False\n",
    "    if torch.is_tensor(kps):\n",
    "        if len(kps) == 3:\n",
    "            print(\">>> View Multiperson\")\n",
    "            multi = True\n",
    "            if kps.shape[1] != 3:\n",
    "                kps = kps.transpose(1,2)\n",
    "        elif len(kps) == 2:\n",
    "            if kps.shape[0] != 3:\n",
    "                kps = kps.transpose(0,1)\n",
    "        else:\n",
    "            raise RuntimeError('Wrong shapes for Kps')\n",
    "    else:\n",
    "        if kps.shape[0] != 3:\n",
    "            kps = kps.T\n",
    "    # kps: bn, 3, NumOfPoints or (3, N)\n",
    "\n",
    "    if ax is None:\n",
    "        print(\"Creating figure >>> \")\n",
    "        fig = plt.figure(figsize =[10,10])\n",
    "        ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "    if kps.shape[1] == 17:\n",
    "        for idx, (i,j) in enumerate(config['coco_tree']):\n",
    "            if multi:\n",
    "                for b in range(kps.shape[0]):\n",
    "                    ax.plot([kps[b][0][i], kps[b][0][j]],\n",
    "                            [kps[b][1][i], kps[b][1][j]],\n",
    "                            [kps[b][2][i], kps[b][2][j]],\n",
    "                            lw=linewidth,\n",
    "                            color=config['color'][idx] if color is None else color,\n",
    "                            alpha=1)\n",
    "            else:\n",
    "                ax.plot([kps[0][i], kps[0][j]], [kps[1][i], kps[1][j]],\n",
    "                        [kps[2][i], kps[2][j]],\n",
    "                        lw=linewidth,\n",
    "                        color=config['color'][idx],\n",
    "                        alpha=1)\n",
    "    else:\n",
    "        for idx, (i,j) in enumerate(config['smpl_tree']):\n",
    "            if multi:\n",
    "                for b in range(kps.shape[0]):\n",
    "                    ax.plot([kps[b][0][i], kps[b][0][j]],\n",
    "                            [kps[b][1][i], kps[b][1][j]],\n",
    "                            [kps[b][2][i], kps[b][2][j]],\n",
    "                            lw=linewidth,\n",
    "                            color=config['smpl_color'][idx] if color is None else color,\n",
    "                            alpha=1)\n",
    "            else:\n",
    "                ax.plot([kps[0][i], kps[0][j]], [kps[1][i], kps[1][j]],\n",
    "                        [kps[2][i], kps[2][j]],\n",
    "                        lw=linewidth,\n",
    "                        color=config['smpl_color'][idx],\n",
    "                        alpha=1)    \n",
    "    \n",
    "    if multi:\n",
    "        for b in range(kps.shape[0]):\n",
    "            ax.scatter(kps[b][0], kps[b][1], kps[b][2], color = 'r', alpha = 1,  marker='o')\n",
    "            #for joint_idx, (x, y, z) in enumerate(zip(kps[b][0], kps[b][1], kps[b][2])):\n",
    "            #    ax.text(x, y, z, str(joint_idx), fontsize=8, color='black', va='center', ha='center')\n",
    "    else:\n",
    "        ax.scatter(kps[0], kps[1], kps[2], color = 'r', s = 100,  marker='o')\n",
    "        #for joint_idx, (x, y, z) in enumerate(zip(kps[0], kps[1], kps[2])):\n",
    "         #   ax.text(x, y, z, str(joint_idx), fontsize=8, color='black', va='center', ha='center')\n",
    "\n",
    "    ax.view_init(phi, theta)\n",
    "    ax.set_xlim(-0.4, max_range)\n",
    "    ax.set_ylim(-0.2, max_range)\n",
    "    ax.set_zlim(-max_range, 0.4)\n",
    "    ax.set_box_aspect((1.,1.0,1.))\n",
    "    \n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "\n",
    "\n",
    "    return fig, ax\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
